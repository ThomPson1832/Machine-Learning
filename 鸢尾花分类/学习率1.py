# 导入TensorFlow库，这是一个用于机器学习和深度学习的开源框架
import tensorflow as tf

# 定义一个可训练的变量w，初始值设为5，数据类型为32位浮点数
# 在梯度下降中，这个变量会不断被优化以找到最优值
w = tf.Variable(tf.constant(5, dtype=tf.float32))

# 设置学习率(learning rate)为0.2
# 学习率控制每次参数更新的幅度，过大会导致不收敛，过小会导致收敛太慢
lr = 0.2

# 设置迭代次数为40次
# 迭代次数表示参数更新的总次数，足够的迭代次数才能让参数收敛到最优值
epoch = 40

# 开始迭代优化过程，循环epoch次
# 每一次循环代表对参数进行一次优化更新
for epoch in range(epoch):
    # 这是TensorFlow中用于记录计算过程的上下文管理器
    # 它会"记录"在其内部进行的所有张量运算，以便后续计算梯度
    with tf.GradientTape() as tape:
        # 定义损失函数(loss function)：(w + 1)的平方
        # 我们的目标是找到使损失函数值最小的w值（理论上是w = -1时，损失为0）
        loss = tf.square(w + 1)

    # 计算损失函数对变量w的梯度(导数)
    # 梯度表示损失函数随w变化的速率和方向，是梯度下降的核心计算
    grads = tape.gradient(loss, w)

    # 使用梯度下降法更新变量w的值
    # assign_sub()方法表示对变量进行自减操作，等价于：w = w - lr * grads
    # 沿着梯度的反方向更新参数，才能使损失函数不断减小
    w.assign_sub(lr * grads)

    # 打印每次迭代后的结果
    # 包括当前迭代次数、w的当前值和对应的损失函数值
    # 可以观察到w逐渐趋近于-1，loss逐渐趋近于0的过程
    print("After %s epoch,w is %f,loss is %f" % (epoch, w.numpy(), loss))

# 学习率可以修改为0.001、0.999等不同值，观察收敛过程的变化：
# - 学习率太小（如0.001）：收敛速度慢，需要更多迭代次数才能接近最优值
# - 学习率太大（如0.999）：可能在最优值附近震荡，甚至无法收敛
# 最终优化目标：找到使损失函数最小的参数w，理论最优解是w = -1