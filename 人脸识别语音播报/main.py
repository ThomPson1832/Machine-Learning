import sys
import os
import cv2
import json
import time
import logging
import numpy as np
from datetime import datetime
from typing import List, Tuple

from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QPushButton, QLabel, QVBoxLayout, QHBoxLayout,
    QWidget, QInputDialog, QComboBox, QSlider, QGroupBox, QFormLayout,
    QTextEdit, QSplitter, QTabWidget, QMessageBox, QProgressBar
)
from PyQt5.QtCore import Qt, QTimer, pyqtSignal, QThread, QMutex, QMutexLocker
from PyQt5.QtGui import QImage, QPixmap, QFont, QPalette, QColor

import face_recognition

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('face_recognition.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)


class VoiceSpeaker:
    """è¯­éŸ³åˆæˆå™¨ï¼ˆè½»é‡ç‰ˆï¼‰"""

    def __init__(self):
        self.engine = None
        self.volume = 0.8
        self.rate = 150
        self.is_speaking = False
        self.speech_mutex = QMutex()
        self.init_engine()

    def init_engine(self):
        """åˆå§‹åŒ–è¯­éŸ³å¼•æ“"""
        try:
            import pyttsx3
            self.engine = pyttsx3.init()
            # ç®€åŒ–è¯­éŸ³é€‰æ‹©ï¼Œå‡å°‘åˆå§‹åŒ–æ—¶é—´
            voices = self.engine.getProperty('voices')
            if voices:
                self.engine.setProperty('voice', voices[0].id)

            self.engine.setProperty('volume', self.volume)
            self.engine.setProperty('rate', self.rate)
        except Exception as e:
            logging.error(f"è¯­éŸ³å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.engine = None

    def set_volume(self, volume):
        """è®¾ç½®éŸ³é‡"""
        self.volume = volume
        if self.engine:
            self.engine.setProperty('volume', volume)

    def set_rate(self, rate):
        """è®¾ç½®è¯­é€Ÿ"""
        self.rate = rate
        if self.engine:
            self.engine.setProperty('rate', rate)

    def speak_face_result(self, names, mode, enable_unknown_alert):
        """æ ¹æ®è¯†åˆ«ç»“æœè¿›è¡Œè¯­éŸ³æ’­æŠ¥"""
        if not self.engine or not names:
            return

        # ç®€åŒ–è¯­éŸ³é€»è¾‘ï¼Œå‡å°‘è®¡ç®—
        known_names = [name for name in names if name != "æœªçŸ¥äººå‘˜"]
        unknown_count = names.count("æœªçŸ¥äººå‘˜")

        if not known_names and not (enable_unknown_alert and unknown_count > 0):
            return

        speak_text = ""
        if known_names:
            speak_text = f"è¯†åˆ«åˆ°{len(known_names)}ä½"
        if enable_unknown_alert and unknown_count > 0:
            speak_text += f"æœªçŸ¥{unknown_count}ä½"

        if speak_text and not self.is_speaking:
            self.speak(speak_text)

    def speak(self, text):
        """è¯­éŸ³æ’­æŠ¥"""
        if not self.engine or self.is_speaking:
            return

        def _speak():
            self.is_speaking = True
            try:
                self.engine.say(text)
                self.engine.runAndWait()
            except Exception as e:
                logging.error(f"è¯­éŸ³æ’­æŠ¥å¤±è´¥: {e}")
            finally:
                self.is_speaking = False

        import threading
        thread = threading.Thread(target=_speak, daemon=True)
        thread.start()

    def stop(self):
        """åœæ­¢è¯­éŸ³æ’­æŠ¥"""
        if self.engine and self.is_speaking:
            try:
                self.engine.stop()
            except:
                pass
            self.is_speaking = False


class FaceRecognizer:
    """äººè„¸è¯†åˆ«å™¨ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰"""

    def __init__(self):
        self.face_database = {}
        self.face_database_path = "face_database.json"
        self.recognition_threshold = 0.6
        self.db_mutex = QMutex()
        self.last_recognition_time = 0
        self.recognition_interval = 1.0  # è¯†åˆ«é—´éš”1ç§’
        self.load_face_database()

    def load_face_database(self):
        """åŠ è½½äººè„¸æ•°æ®åº“"""
        try:
            if os.path.exists(self.face_database_path):
                with QMutexLocker(self.db_mutex):
                    with open(self.face_database_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        for name, encoding in data.items():
                            self.face_database[name] = np.array(encoding)
                logging.info(f"åŠ è½½äººè„¸æ•°æ®åº“æˆåŠŸï¼Œå…± {len(self.face_database)} äºº")
        except Exception as e:
            logging.error(f"åŠ è½½äººè„¸æ•°æ®åº“å¤±è´¥: {e}")
            self.face_database = {}

    def save_face_database(self):
        """ä¿å­˜äººè„¸æ•°æ®åº“"""
        try:
            with QMutexLocker(self.db_mutex):
                data = {name: encoding.tolist() for name, encoding in self.face_database.items()}
                with open(self.face_database_path, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"ä¿å­˜äººè„¸æ•°æ®åº“å¤±è´¥: {e}")

    def fast_detect_faces(self, frame):
        """å¿«é€Ÿäººè„¸æ£€æµ‹"""
        try:
            # ä½¿ç”¨å°å°ºå¯¸å›¾åƒè¿›è¡Œå¿«é€Ÿæ£€æµ‹
            small_frame = cv2.resize(frame, (320, 240))
            rgb_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)

            # ä½¿ç”¨HOGæ¨¡å‹ï¼ˆæœ€å¿«ï¼‰
            face_locations = face_recognition.face_locations(
                rgb_frame,
                number_of_times_to_upsample=0,  # ä¸è¿›è¡Œä¸Šé‡‡æ ·
                model="hog"
            )

            # è½¬æ¢åæ ‡æ ¼å¼
            converted_locations = []
            for (top, right, bottom, left) in face_locations:
                # ç¼©æ”¾å›åŸå›¾åæ ‡
                scale_x = frame.shape[1] / 320
                scale_y = frame.shape[0] / 240
                x = int(left * scale_x)
                y = int(top * scale_y)
                w = int((right - left) * scale_x)
                h = int((bottom - top) * scale_y)
                converted_locations.append((x, y, w, h))

            return converted_locations

        except Exception as e:
            logging.error(f"å¿«é€Ÿäººè„¸æ£€æµ‹å¤±è´¥: {e}")
            return []

    def recognize_fast(self, frame):
        """å¿«é€Ÿè¯†åˆ«äººè„¸"""
        try:
            current_time = time.time()
            if current_time - self.last_recognition_time < self.recognition_interval:
                return []

            self.last_recognition_time = current_time

            # å¿«é€Ÿæ£€æµ‹äººè„¸
            face_locations = self.fast_detect_faces(frame)
            if not face_locations:
                return []

            # ä½¿ç”¨å°å°ºå¯¸å›¾åƒæå–ç‰¹å¾
            small_frame = cv2.resize(frame, (640, 480))
            rgb_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)

            # è½¬æ¢åæ ‡åˆ°å°å°ºå¯¸
            small_face_locations = []
            for (x, y, w, h) in face_locations:
                scale_x = 640 / frame.shape[1]
                scale_y = 480 / frame.shape[0]
                small_top = int(y * scale_y)
                small_right = int((x + w) * scale_x)
                small_bottom = int((y + h) * scale_y)
                small_left = int(x * scale_x)
                small_face_locations.append((small_top, small_right, small_bottom, small_left))

            # æå–ç‰¹å¾
            face_encodings = face_recognition.face_encodings(rgb_frame, small_face_locations)

            results = []
            with QMutexLocker(self.db_mutex):
                for i, (location, encoding) in enumerate(zip(face_locations, face_encodings)):
                    x, y, w, h = location

                    name = "æœªçŸ¥äººå‘˜"
                    min_distance = float('inf')

                    # å¿«é€Ÿæ¯”å¯¹ï¼ˆé™åˆ¶æœ€å¤§æ¯”å¯¹æ¬¡æ•°ï¼‰
                    max_compares = min(10, len(self.face_database))  # æœ€å¤šæ¯”å¯¹10ä¸ª
                    db_items = list(self.face_database.items())[:max_compares]

                    for db_name, db_encoding in db_items:
                        distance = face_recognition.face_distance([db_encoding], encoding)[0]
                        if distance < min_distance and distance < self.recognition_threshold:
                            min_distance = distance
                            name = db_name

                    results.append((name, (x, y, w, h), min_distance))

            return results
        except Exception as e:
            logging.error(f"å¿«é€Ÿäººè„¸è¯†åˆ«å¤±è´¥: {e}")
            return []

    def get_database_info(self):
        """è·å–æ•°æ®åº“ä¿¡æ¯"""
        with QMutexLocker(self.db_mutex):
            return {
                "total_faces": len(self.face_database),
                "names": list(self.face_database.keys()),
                "last_update": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }


class RecognitionThread(QThread):
    """äººè„¸è¯†åˆ«çº¿ç¨‹ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰"""
    recognition_complete = pyqtSignal(list)

    def __init__(self, recognizer, frame):
        super().__init__()
        self.recognizer = recognizer
        self.frame = frame.copy()
        self.setTerminationEnabled(True)

    def run(self):
        try:
            results = self.recognizer.recognize_fast(self.frame)
            self.recognition_complete.emit(results)
        except Exception as e:
            logging.error(f"è¯†åˆ«çº¿ç¨‹é”™è¯¯: {e}")
            self.recognition_complete.emit([])
        finally:
            # é‡Šæ”¾å†…å­˜
            del self.frame


class CaptureFaceThread(QThread):
    """äººè„¸é‡‡é›†çº¿ç¨‹ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰"""
    capture_complete = pyqtSignal(bool, str)
    capture_progress = pyqtSignal(int)

    def __init__(self, recognizer, frame, name):
        super().__init__()
        self.recognizer = recognizer
        self.frame = frame.copy()
        self.name = name.strip()
        self.setTerminationEnabled(True)

    def run(self):
        try:
            self.capture_progress.emit(20)

            # å¿«é€Ÿäººè„¸æ£€æµ‹
            face_locations = self.recognizer.fast_detect_faces(self.frame)
            self.capture_progress.emit(40)

            if len(face_locations) == 0:
                self.capture_complete.emit(False, "æœªæ£€æµ‹åˆ°äººè„¸")
                return
            elif len(face_locations) > 1:
                self.capture_complete.emit(False, f"æ£€æµ‹åˆ° {len(face_locations)} ä¸ªäººè„¸")
                return

            self.capture_progress.emit(60)

            # æå–ç‰¹å¾
            rgb_frame = cv2.cvtColor(self.frame, cv2.COLOR_BGR2RGB)
            x, y, w, h = face_locations[0]
            face_recognition_location = [(y, x + w, y + h, x)]

            try:
                face_encodings = face_recognition.face_encodings(rgb_frame, face_recognition_location)
                self.capture_progress.emit(80)
            except Exception as e:
                logging.error(f"äººè„¸ç‰¹å¾æå–å¤±è´¥: {e}")
                self.capture_complete.emit(False, f"ç‰¹å¾æå–å¤±è´¥: {str(e)}")
                return

            if not face_encodings:
                self.capture_complete.emit(False, "æ— æ³•æå–äººè„¸ç‰¹å¾")
                return

            # å®‰å…¨ä¿å­˜åˆ°æ•°æ®åº“
            try:
                self.capture_progress.emit(90)  # æ·»åŠ ä¸­é—´è¿›åº¦
                
                # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
                db_path = self.recognizer.face_database_path
                if not os.path.exists(db_path):
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    db_dir = os.path.dirname(db_path)
                    if db_dir and not os.path.exists(db_dir):
                        os.makedirs(db_dir)
                    
                    # åˆ›å»ºç©ºçš„æ•°æ®åº“æ–‡ä»¶
                    with open(db_path, 'w', encoding='utf-8') as f:
                        json.dump({}, f)
                    logging.info("åˆ›å»ºäº†æ–°çš„æ•°æ®åº“æ–‡ä»¶")

                # æ·»åŠ å§“åå‰ç¼€æ£€æŸ¥
                safe_name = self.name.strip()
                if not safe_name:
                    self.capture_complete.emit(False, "å§“åä¸èƒ½ä¸ºç©º")
                    return

                # æ£€æŸ¥é‡å¤å§“å - ä¸´æ—¶è·å–é”æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                temp_mutex = QMutexLocker(self.recognizer.db_mutex)
                name_exists = safe_name in self.recognizer.face_database
                temp_mutex.unlock()
                
                if name_exists:
                    # è¯¢é—®æ˜¯å¦è¦†ç›–
                    self.capture_complete.emit(False, f"å§“å '{safe_name}' å·²å­˜åœ¨")
                    return
                
                # ä¿å­˜åˆ°å†…å­˜æ•°æ®åº“ - è·å–é”
                with QMutexLocker(self.recognizer.db_mutex):
                    self.recognizer.face_database[safe_name] = face_encodings[0]
                    
                    # ç›´æ¥ä¿å­˜æ–‡ä»¶ï¼Œé¿å…æ­»é”
                    try:
                        data = {name: encoding.tolist() for name, encoding in self.recognizer.face_database.items()}
                        with open(db_path, "w", encoding="utf-8") as f:
                            json.dump(data, f, ensure_ascii=False, indent=2)
                    except Exception as save_e:
                        # å¦‚æœä¿å­˜å¤±è´¥ï¼Œæ’¤é”€å†…å­˜ä¸­çš„æ•°æ®
                        del self.recognizer.face_database[safe_name]
                        logging.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ï¼Œå·²æ’¤é”€æ•°æ®: {save_e}")
                        raise save_e
                
                self.capture_progress.emit(100)
                self.capture_complete.emit(True, f"æˆåŠŸé‡‡é›† {safe_name} çš„äººè„¸ç‰¹å¾")

            except PermissionError as e:
                logging.error(f"æ•°æ®åº“æ–‡ä»¶æƒé™é”™è¯¯: {e}")
                self.capture_complete.emit(False, "æ•°æ®åº“æ–‡ä»¶æƒé™ä¸è¶³ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å±æ€§")
            except json.JSONDecodeError as e:
                logging.error(f"JSONæ ¼å¼é”™è¯¯: {e}")
                # å¤‡ä»½å¹¶é‡å»ºæ•°æ®åº“
                backup_path = f"{self.recognizer.face_database_path}.backup"
                try:
                    if os.path.exists(self.recognizer.face_database_path):
                        import shutil
                        shutil.copy2(self.recognizer.face_database_path, backup_path)
                    
                    # é‡å»ºæ•°æ®åº“ - ä½¿ç”¨ä¸´æ—¶æ•°æ®é¿å…æ­»é”
                    temp_data = {safe_name: face_encodings[0].tolist()}
                    with open(self.recognizer.face_database_path, 'w', encoding='utf-8') as f:
                        json.dump(temp_data, f, ensure_ascii=False, indent=2)
                    
                    # æ›´æ–°å†…å­˜æ•°æ®åº“
                    with QMutexLocker(self.recognizer.db_mutex):
                        self.recognizer.face_database[safe_name] = face_encodings[0]
                    
                    self.capture_progress.emit(100)
                    self.capture_complete.emit(True, f"æˆåŠŸé‡‡é›† {safe_name} çš„äººè„¸ç‰¹å¾ï¼ˆæ•°æ®åº“å·²é‡å»ºï¼‰")
                    logging.info(f"æ•°æ®åº“å·²é‡å»ºï¼ŒåŸæ–‡ä»¶å¤‡ä»½ä¸º {backup_path}")
                    
                except Exception as backup_e:
                    logging.error(f"æ•°æ®åº“é‡å»ºå¤±è´¥: {backup_e}")
                    self.capture_complete.emit(False, "æ•°æ®åº“æ–‡ä»¶æŸåä¸”ä¿®å¤å¤±è´¥")
            
            except Exception as e:
                logging.error(f"ä¿å­˜æ•°æ®åº“å¤±è´¥: {e}")
                self.capture_complete.emit(False, f"ä¿å­˜å¤±è´¥: {str(e)}")

        except Exception as e:
            logging.error(f"é‡‡é›†äººè„¸å¤±è´¥: {e}")
            self.capture_complete.emit(False, f"é‡‡é›†å¤±è´¥: {str(e)}")
        finally:
            # é‡Šæ”¾å†…å­˜
            del self.frame


class FaceRecognitionApp(QMainWindow):
    """ä¸»åº”ç”¨ç¨‹åºï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰"""

    def __init__(self):
        super().__init__()
        self.setWindowTitle("ğŸ­ æ™ºèƒ½è¯­éŸ³æ’­æŠ¥äººè„¸è¯†åˆ«ç³»ç»Ÿ - æµç•…ç‰ˆ")
        self.setGeometry(100, 100, 1200, 800)  # ç¼©å°çª—å£å°ºå¯¸

        # åˆå§‹åŒ–ç»„ä»¶
        self.recognizer = FaceRecognizer()
        self.speaker = VoiceSpeaker()

        # çŠ¶æ€å˜é‡
        self.is_recognizing = False
        self.is_camera_available = False
        self.last_speak_time = {}
        self.recognition_results = []
        self.frame_mutex = QMutex()
        self.current_frame = None
        self.current_recognition_thread = None
        self.frame_skip_counter = 0  # å¸§è·³è¿‡è®¡æ•°å™¨
        self.frame_skip_interval = 2  # æ¯3å¸§å¤„ç†1å¸§

        # åŠ è½½é…ç½®
        self.config = self.load_config()
        self.apply_config()

        # åˆå§‹åŒ–UI
        self.init_ui()
        self.init_camera()

        # å®šæ—¶å™¨
        self.timer = QTimer()
        self.timer.timeout.connect(self.process_frame)

        logging.info("åº”ç”¨ç¨‹åºåˆå§‹åŒ–å®Œæˆ")

    def load_config(self):
        """åŠ è½½é…ç½®"""
        try:
            with open("config.json", "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return self.get_default_config()

    def get_default_config(self):
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "mode": "ç®€æ´æ¨¡å¼",
            "volume": 0.8,
            "rate": 150,
            "enable_unknown_alert": True,
            "speak_cooldown": 5,
            "camera_index": 0,
            "recognition_threshold": 0.6,
            "performance_mode": "å¹³è¡¡"  # æ–°å¢æ€§èƒ½æ¨¡å¼
        }

    def save_config(self):
        """ä¿å­˜é…ç½®"""
        try:
            with open("config.json", "w", encoding="utf-8") as f:
                json.dump(self.config, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")

    def apply_config(self):
        """åº”ç”¨é…ç½®"""
        self.speaker.set_volume(self.config.get('volume', 0.8))
        self.speaker.set_rate(self.config.get('rate', 150))

        self.speak_cooldown = self.config.get('speak_cooldown', 5)
        self.enable_unknown_alert = self.config.get('enable_unknown_alert', True)
        self.current_mode = self.config.get('mode', 'ç®€æ´æ¨¡å¼')
        self.recognizer.recognition_threshold = self.config.get('recognition_threshold', 0.6)

        # æ ¹æ®æ€§èƒ½æ¨¡å¼è°ƒæ•´å‚æ•°
        performance_mode = self.config.get('performance_mode', 'å¹³è¡¡')
        if performance_mode == 'æµç•…':
            self.frame_skip_interval = 3
            self.recognizer.recognition_interval = 1.5
        elif performance_mode == 'å¹³è¡¡':
            self.frame_skip_interval = 2
            self.recognizer.recognition_interval = 1.0
        else:  # ç²¾å‡†
            self.frame_skip_interval = 1
            self.recognizer.recognition_interval = 0.5

    def init_ui(self):
        """åˆå§‹åŒ–ç”¨æˆ·ç•Œé¢ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        self.setStyleSheet("""
            QMainWindow { 
                background-color: #2b2b2b; 
                color: white; 
            }
            QPushButton {
                background-color: #4CAF50; 
                color: white; 
                border: none;
                padding: 6px 12px; 
                border-radius: 4px; 
                font-size: 12px;
            }
            QPushButton:hover { background-color: #45a049; }
            QPushButton:pressed { background-color: #3d8b40; }
            QLabel { color: white; font-size: 12px; }
            QComboBox, QSlider {
                background-color: #3c3c3c; 
                color: white;
                border: 1px solid #555; 
                border-radius: 4px;
            }
            QGroupBox {
                color: #4CAF50; 
                font-weight: bold;
                border: 1px solid #4CAF50; 
                border-radius: 6px;
                margin-top: 8px; 
                padding-top: 8px;
            }
            QTextEdit {
                background-color: #1e1e1e; 
                color: #00ff00;
                border: 1px solid #555; 
                border-radius: 4px;
                font-family: 'Courier New';
                font-size: 10px;
            }
            QProgressBar {
                height: 8px; 
                border-radius: 4px; 
                background-color: #333333;
            }
            QProgressBar::chunk { 
                background-color: #4CAF50; 
                border-radius: 4px; 
            }
        """)

        # ä¸»å¸ƒå±€
        main_widget = QWidget()
        self.setCentralWidget(main_widget)
        layout = QHBoxLayout(main_widget)

        # å·¦ä¾§è§†é¢‘åŒºåŸŸ
        left_widget = QWidget()
        left_layout = QVBoxLayout(left_widget)

        # è§†é¢‘æ˜¾ç¤º
        self.video_label = QLabel("æ‘„åƒå¤´æœªå¯åŠ¨")
        self.video_label.setAlignment(Qt.AlignCenter)
        self.video_label.setMinimumSize(640, 480)  # ç¼©å°æ˜¾ç¤ºå°ºå¯¸
        self.video_label.setStyleSheet("""
            QLabel { 
                background: black; 
                color: white; 
                font-size: 14px; 
                border: 1px solid #555; 
            }
        """)

        # æ§åˆ¶æŒ‰é’®
        control_layout = QHBoxLayout()
        self.recognize_btn = QPushButton("å¼€å§‹è¯†åˆ«")
        self.capture_btn = QPushButton("é‡‡é›†äººè„¸")
        self.clear_db_btn = QPushButton("æ¸…ç©ºæ•°æ®åº“")
        control_layout.addWidget(self.recognize_btn)
        control_layout.addWidget(self.capture_btn)
        control_layout.addWidget(self.clear_db_btn)

        # é‡‡é›†è¿›åº¦æ¡
        self.capture_progress_bar = QProgressBar()
        self.capture_progress_bar.setRange(0, 100)
        self.capture_progress_bar.setVisible(False)

        # çŠ¶æ€æ˜¾ç¤º
        self.status_label = QLabel("ğŸ”´ ç³»ç»Ÿå°±ç»ª")
        self.status_label.setStyleSheet("QLabel { font-size: 14px; color: #ff6b6b; }")

        left_layout.addWidget(self.video_label)
        left_layout.addLayout(control_layout)
        left_layout.addWidget(self.capture_progress_bar)
        left_layout.addWidget(self.status_label)

        # å³ä¾§æ§åˆ¶é¢æ¿ï¼ˆç®€åŒ–ï¼‰
        right_widget = QWidget()
        right_layout = QVBoxLayout(right_widget)
        right_layout.setAlignment(Qt.AlignTop)

        # æ€§èƒ½è®¾ç½®
        perf_group = QGroupBox("âš¡ æ€§èƒ½è®¾ç½®")
        perf_layout = QFormLayout(perf_group)
        self.performance_combo = QComboBox()
        self.performance_combo.addItems(["æµç•…", "å¹³è¡¡", "ç²¾å‡†"])
        self.performance_combo.setCurrentText(self.config.get('performance_mode', 'å¹³è¡¡'))
        perf_layout.addRow("æ€§èƒ½æ¨¡å¼:", self.performance_combo)

        # è¯­éŸ³è®¾ç½®
        voice_group = QGroupBox("ğŸµ è¯­éŸ³è®¾ç½®")
        voice_layout = QFormLayout(voice_group)
        self.mode_combo = QComboBox()
        self.mode_combo.addItems(["ç®€æ´æ¨¡å¼", "é™éŸ³æ¨¡å¼"])
        self.mode_combo.setCurrentText(self.current_mode)
        voice_layout.addRow("æ’­æŠ¥æ¨¡å¼:", self.mode_combo)

        # è¯†åˆ«è®¾ç½®
        recog_group = QGroupBox("ğŸ” è¯†åˆ«è®¾ç½®")
        recog_layout = QFormLayout(recog_group)
        self.threshold_slider = QSlider(Qt.Horizontal)
        self.threshold_slider.setRange(4, 8)  # 0.4-0.8
        self.threshold_slider.setValue(int(self.config.get('recognition_threshold', 0.6) * 10))
        recog_layout.addRow("è¯†åˆ«é˜ˆå€¼:", self.threshold_slider)

        # æ•°æ®åº“ä¿¡æ¯
        db_group = QGroupBox("ğŸ’¾ æ•°æ®åº“ä¿¡æ¯")
        db_layout = QVBoxLayout(db_group)
        self.db_info_label = QLabel("åŠ è½½ä¸­...")
        self.db_info_label.setWordWrap(True)
        db_layout.addWidget(self.db_info_label)

        # ç³»ç»Ÿæ—¥å¿—
        log_group = QGroupBox("ğŸ“ ç³»ç»Ÿæ—¥å¿—")
        log_layout = QVBoxLayout(log_group)
        self.log_text = QTextEdit()
        self.log_text.setMaximumHeight(150)
        self.log_text.setReadOnly(True)
        log_layout.addWidget(self.log_text)

        right_layout.addWidget(perf_group)
        right_layout.addWidget(voice_group)
        right_layout.addWidget(recog_group)
        right_layout.addWidget(db_group)
        right_layout.addWidget(log_group)

        # åˆ†å‰²å¸ƒå±€
        splitter = QSplitter(Qt.Horizontal)
        splitter.addWidget(left_widget)
        splitter.addWidget(right_widget)
        splitter.setSizes([600, 300])
        layout.addWidget(splitter)

        # è¿æ¥ä¿¡å·
        self.connect_signals()
        self.update_database_info()

    def connect_signals(self):
        """è¿æ¥ä¿¡å·æ§½"""
        self.recognize_btn.clicked.connect(self.toggle_recognition)
        self.capture_btn.clicked.connect(self.capture_face)
        self.clear_db_btn.clicked.connect(self.clear_database)

        self.performance_combo.currentTextChanged.connect(self.on_performance_change)
        self.mode_combo.currentTextChanged.connect(self.on_mode_change)
        self.threshold_slider.valueChanged.connect(self.on_threshold_change)

    def init_camera(self):
        """åˆå§‹åŒ–æ‘„åƒå¤´ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰"""
        try:
            camera_index = self.config.get('camera_index', 0)
            self.cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)

            # é™ä½åˆ†è¾¨ç‡æé«˜æ€§èƒ½
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            self.cap.set(cv2.CAP_PROP_FPS, 15)  # é™ä½å¸§ç‡
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # æœ€å°åŒ–ç¼“å†²åŒº
            self.cap.set(cv2.CAP_PROP_AUTOFOCUS, 0)  # å…³é—­è‡ªåŠ¨å¯¹ç„¦

            if self.cap.isOpened():
                self.is_camera_available = True
                self.log_message("âœ… æ‘„åƒå¤´åˆå§‹åŒ–æˆåŠŸï¼ˆ640x480ï¼‰")
            else:
                self.is_camera_available = False
                self.log_message("âŒ æ‘„åƒå¤´åˆå§‹åŒ–å¤±è´¥")

        except Exception as e:
            self.is_camera_available = False
            self.log_message(f"âŒ æ‘„åƒå¤´åˆå§‹åŒ–é”™è¯¯: {e}")

    def toggle_recognition(self):
        """åˆ‡æ¢è¯†åˆ«çŠ¶æ€"""
        if not self.is_camera_available:
            QMessageBox.warning(self, "è­¦å‘Š", "æ‘„åƒå¤´ä¸å¯ç”¨")
            return

        if not self.is_recognizing:
            self.is_recognizing = True
            self.recognize_btn.setText("åœæ­¢è¯†åˆ«")
            self.recognize_btn.setStyleSheet("background-color: #ff6b6b;")
            self.status_label.setText("ğŸŸ¢ è¯†åˆ«ä¸­...")
            self.timer.start(67)  # ~15 FPS
            self.log_message("ğŸ¯ å¼€å§‹äººè„¸è¯†åˆ«")
        else:
            self.stop_recognition()

    def stop_recognition(self):
        """åœæ­¢è¯†åˆ«"""
        self.is_recognizing = False
        self.recognize_btn.setText("å¼€å§‹è¯†åˆ«")
        self.recognize_btn.setStyleSheet("background-color: #4CAF50;")
        self.status_label.setText("ğŸ”´ è¯†åˆ«å·²åœæ­¢")
        self.timer.stop()

        if self.current_recognition_thread and self.current_recognition_thread.isRunning():
            self.current_recognition_thread.quit()
            self.current_recognition_thread.wait(500)

        self.current_recognition_thread = None
        self.log_message("â¹ï¸ åœæ­¢äººè„¸è¯†åˆ«")

    def process_frame(self):
        """å¤„ç†è§†é¢‘å¸§ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰"""
        if not self.is_camera_available:
            return

        ret, frame = self.cap.read()
        if not ret:
            return

        frame = cv2.flip(frame, 1)

        # å¸§è·³è¿‡æœºåˆ¶ï¼Œå‡å°‘å¤„ç†é¢‘ç‡
        self.frame_skip_counter += 1
        if self.frame_skip_counter < self.frame_skip_interval:
            # åªæ˜¾ç¤ºï¼Œä¸å¤„ç†è¯†åˆ«
            self.display_frame_fast(frame, self.recognition_results)
            return

        self.frame_skip_counter = 0

        with QMutexLocker(self.frame_mutex):
            self.current_frame = frame.copy()

        # æ˜¾ç¤ºå¸§
        self.display_frame_fast(frame, self.recognition_results)

        if self.is_recognizing:
            # ç¡®ä¿åªæœ‰ä¸€ä¸ªè¯†åˆ«çº¿ç¨‹åœ¨è¿è¡Œ
            if (self.current_recognition_thread is None or
                    not self.current_recognition_thread.isRunning()):
                with QMutexLocker(self.frame_mutex):
                    thread_frame = self.current_frame.copy() if self.current_frame is not None else frame

                self.current_recognition_thread = RecognitionThread(self.recognizer, thread_frame)
                self.current_recognition_thread.recognition_complete.connect(self.on_recognition_complete)
                self.current_recognition_thread.start()

    def on_recognition_complete(self, results):
        """è¯†åˆ«å®Œæˆå›è°ƒ"""
        self.recognition_results = results
        self.handle_voice_announce(results)

    def display_frame_fast(self, frame, results=None):
        """å¿«é€Ÿæ˜¾ç¤ºè§†é¢‘å¸§ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰"""
        try:
            display_frame = frame.copy()

            if results:
                # ç®€åŒ–ç»˜åˆ¶é€»è¾‘
                for name, bbox, confidence in results:
                    x, y, w, h = bbox
                    color = (0, 0, 255) if name == "æœªçŸ¥äººå‘˜" else (0, 255, 0)

                    # åªç»˜åˆ¶çŸ©å½¢ï¼Œä¸ç»˜åˆ¶æ–‡æœ¬ï¼ˆå‡å°‘è®¡ç®—ï¼‰
                    cv2.rectangle(display_frame, (x, y), (x + w, y + h), color, 2)

                    # åªåœ¨ç²¾å‡†æ¨¡å¼ä¸‹ç»˜åˆ¶æ–‡æœ¬
                    if self.config.get('performance_mode', 'å¹³è¡¡') == 'ç²¾å‡†':
                        label = f"{name}"
                        cv2.putText(display_frame, label, (x, y - 5),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            # å¿«é€Ÿè½¬æ¢å›¾åƒæ ¼å¼
            rgb_frame = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
            h, w, ch = rgb_frame.shape
            bytes_per_line = ch * w

            # ä½¿ç”¨æ›´å¿«çš„å›¾åƒç¼©æ”¾
            qt_image = QImage(rgb_frame.data, w, h, bytes_per_line, QImage.Format_RGB888)

            if not qt_image.isNull():
                # ä½¿ç”¨å¿«é€Ÿç¼©æ”¾
                pixmap = QPixmap.fromImage(qt_image).scaled(
                    self.video_label.width(),
                    self.video_label.height(),
                    Qt.KeepAspectRatio,
                    Qt.FastTransformation  # ä½¿ç”¨å¿«é€Ÿå˜æ¢
                )
                self.video_label.setPixmap(pixmap)

        except Exception as e:
            # ç®€åŒ–é”™è¯¯å¤„ç†
            pass

    def capture_face(self):
        """é‡‡é›†äººè„¸"""
        if not self.is_camera_available:
            QMessageBox.warning(self, "è­¦å‘Š", "æ‘„åƒå¤´ä¸å¯ç”¨")
            return

        # åœæ­¢è¯†åˆ«ä»¥ç¡®ä¿ç¨³å®šæ€§
        if self.is_recognizing:
            self.stop_recognition()
            QTimer.singleShot(300, self._do_capture)
        else:
            self._do_capture()

    def _do_capture(self):
        """æ‰§è¡Œé‡‡é›†æ“ä½œ"""
        ret, frame = self.cap.read()
        if not ret:
            QMessageBox.warning(self, "é”™è¯¯", "æ— æ³•è·å–æ‘„åƒå¤´ç”»é¢")
            return

        frame = cv2.flip(frame, 1)

        # å¿«é€Ÿæ£€æµ‹äººè„¸
        face_locations = self.recognizer.fast_detect_faces(frame)

        if len(face_locations) == 0:
            QMessageBox.warning(self, "æç¤º", "æœªæ£€æµ‹åˆ°äººè„¸")
            return
        elif len(face_locations) > 1:
            QMessageBox.warning(self, "æç¤º", "æ£€æµ‹åˆ°å¤šä¸ªäººè„¸")
            return

        name, ok = QInputDialog.getText(self, "é‡‡é›†äººè„¸", "è¯·è¾“å…¥å§“å:")
        if ok and name.strip():
            self.capture_progress_bar.setVisible(True)
            self.capture_progress_bar.setValue(0)

            self.capture_thread = CaptureFaceThread(self.recognizer, frame, name)
            self.capture_thread.capture_complete.connect(self.on_capture_complete)
            self.capture_thread.capture_progress.connect(self.on_capture_progress)
            self.capture_thread.start()

    def on_capture_progress(self, value):
        """æ›´æ–°é‡‡é›†è¿›åº¦"""
        self.capture_progress_bar.setValue(value)

    def on_capture_complete(self, success, message):
        """é‡‡é›†å®Œæˆå›è°ƒ"""
        self.capture_progress_bar.setVisible(False)
        self.capture_progress_bar.setValue(0)

        if success:
            self.log_message(f"âœ… {message}")
            self.update_database_info()
            # ç®€åŒ–æˆåŠŸæç¤º
            QMessageBox.information(self, "æˆåŠŸ", message)
        else:
            self.log_message(f"âŒ {message}")
            QMessageBox.warning(self, "å¤±è´¥", message)

    def handle_voice_announce(self, results):
        """å¤„ç†è¯­éŸ³æ’­æŠ¥"""
        if not results or self.current_mode == "é™éŸ³æ¨¡å¼":
            return

        current_time = time.time()
        names_to_speak = []

        for name, _, confidence in results:
            last_time = self.last_speak_time.get(name, 0)
            if current_time - last_time > self.speak_cooldown:
                names_to_speak.append(name)
                self.last_speak_time[name] = current_time

        if names_to_speak:
            self.speaker.speak_face_result(names_to_speak, self.current_mode, self.enable_unknown_alert)

    def clear_database(self):
        """æ¸…ç©ºæ•°æ®åº“"""
        reply = QMessageBox.question(self, "ç¡®è®¤", "ç¡®å®šè¦æ¸…ç©ºæ‰€æœ‰äººè„¸æ•°æ®å—ï¼Ÿ",
                                     QMessageBox.Yes | QMessageBox.No)
        if reply == QMessageBox.Yes:
            self.recognizer.face_database = {}
            self.recognizer.save_face_database()
            self.update_database_info()
            self.log_message("ğŸ—‘ï¸ äººè„¸æ•°æ®åº“å·²æ¸…ç©º")

    def update_database_info(self):
        """æ›´æ–°æ•°æ®åº“ä¿¡æ¯"""
        db_info = self.recognizer.get_database_info()
        info_text = f"æ€»äººæ•°: {db_info['total_faces']}\n"
        if db_info['names']:
            info_text += f"åå•: {', '.join(db_info['names'][:3])}"  # åªæ˜¾ç¤ºå‰3ä¸ª
            if len(db_info['names']) > 3:
                info_text += "..."
        self.db_info_label.setText(info_text)

    def log_message(self, message):
        """è®°å½•æ—¥å¿—ï¼ˆç®€åŒ–ï¼‰"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self.log_text.append(log_entry)
        # é™åˆ¶æ—¥å¿—é•¿åº¦
        if self.log_text.document().lineCount() > 50:
            cursor = self.log_text.textCursor()
            cursor.movePosition(cursor.Start)
            cursor.movePosition(cursor.Down, cursor.KeepAnchor, 10)
            cursor.removeSelectedText()

    def on_performance_change(self, mode):
        """æ€§èƒ½æ¨¡å¼æ”¹å˜"""
        self.config['performance_mode'] = mode
        self.save_config()
        self.apply_config()
        self.log_message(f"âš¡ æ€§èƒ½æ¨¡å¼: {mode}")

    def on_mode_change(self, mode):
        self.current_mode = mode
        self.config['mode'] = mode
        self.save_config()

    def on_threshold_change(self, value):
        threshold = value / 10.0
        self.recognizer.recognition_threshold = threshold
        self.config['recognition_threshold'] = threshold
        self.save_config()

    def closeEvent(self, event):
        """å®‰å…¨å…³é—­åº”ç”¨ç¨‹åº"""
        self.stop_recognition()

        if hasattr(self, 'cap'):
            self.cap.release()

        if hasattr(self, 'speaker'):
            self.speaker.stop()

        self.save_config()
        event.accept()


if __name__ == "__main__":
    # è®¾ç½®é«˜æ€§èƒ½æ¨¡å¼
    import os

    os.environ["OMP_NUM_THREADS"] = "1"
    os.environ["OPENBLAS_NUM_THREADS"] = "1"

    app = QApplication(sys.argv)
    app.setApplicationName("äººè„¸è¯†åˆ«ç³»ç»Ÿ - æµç•…ç‰ˆ")


    # ç®€åŒ–å¼‚å¸¸å¤„ç†
    def exception_handler(exctype, value, traceback):
        logging.error(f"å¼‚å¸¸: {exctype.__name__}: {value}")


    sys.excepthook = exception_handler

    window = FaceRecognitionApp()
    window.show()

    try:
        sys.exit(app.exec_())
    except Exception as e:
        logging.error(f"åº”ç”¨ç¨‹åºé€€å‡º: {e}")


